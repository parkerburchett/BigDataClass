{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Big Data Lab3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPt5q27L5557"
      },
      "source": [
        "# Colab 3: Implement Bloom Filter using Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0-YhEpP_Ds-"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbYZoVVWOZA5"
      },
      "source": [
        "Let's setup Spark on your Colab environment.  Run the cell below!\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhzk3GE6S9RC",
        "outputId": "cd2fba8e-4750-42fc-8f65-a104cd19ae8e"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.0.2)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9)\n",
            "openjdk-8-jdk-headless is already the newest version (8u282-b08-0ubuntu1~18.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 11 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv_aXQDamh3p"
      },
      "source": [
        "#Import libraries\n",
        "import hashlib\n",
        "import math\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "\n",
        "# create the context\n",
        "try:\n",
        "  sc.stop()\n",
        "  sc = pyspark.SparkContext(conf=conf)\n",
        "except:\n",
        "  sc = pyspark.SparkContext(conf=conf)\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAYRX2PMm0L6"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO_IcxgquzhI"
      },
      "source": [
        "From the NLTK (Natural Language ToolKit) library, we import a large list of English dictionary words, commonly used by the spell-checking programs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Xz3f79crEEb",
        "outputId": "cb7f3c35-0bbc-4828-ade6-0042fce7ec24"
      },
      "source": [
        "import nltk\n",
        "nltk.download('words')\n",
        "\n",
        "from nltk.corpus import words\n",
        "word_list = words.words()\n",
        "\n",
        "# Convert word_list to a Spark RDD\n",
        "word_rdd = spark.sparkContext.parallelize(word_list)\n"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPRqEqNp8nbH"
      },
      "source": [
        "## Determine the size of the hash table\n",
        "In order to build a Bloom Filter, we need to determine the size of the hash table which is calcuated based on the number of words in the dictionary and the number of hash funcitons we plan to use. The following `get_digits` function returns the number of digits we need to take out of each hash function to get the indices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rML_6evN8wk0"
      },
      "source": [
        "def get_digits(n, hash_num):\n",
        "\n",
        "    # m gives how many indices we need\n",
        "    m = hash_num * n / (math.log(2))\n",
        "    # Number of digits we need to take out of each hash function to get the indices\n",
        "    num_of_digits = len(str(int(m)))\n",
        "\n",
        "    return num_of_digits"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rd234Yym9LVg",
        "outputId": "672ba82c-6472-4ee2-ea97-c7662e875e03"
      },
      "source": [
        "# get the total number of words in the dictionary\n",
        "wordCount = word_rdd.count()\n",
        "print(wordCount)\n",
        "# set the number of hash functions to 4\n",
        "NUM_HASHFUNCTIONS = 4\n",
        "\n",
        "num_of_digits = get_digits(wordCount, NUM_HASHFUNCTIONS)\n",
        "print(num_of_digits)\n",
        "print(math.pow(10,num_of_digits))"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "236736\n",
            "7\n",
            "10000000.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiRnERN0__U2"
      },
      "source": [
        "## Get Hash Values\n",
        "The `hash_values` funciton takes a key and the `num_of_digits` as inputs, and returns 4 indices using 4 secure hash functions from Python [hashlib](https://docs.python.org/3/library/hashlib.html) library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLosSh75Lt8D"
      },
      "source": [
        "def hash_values(item, num_of_digits):\n",
        "  # Get the index value to check for the four hash functions we are using\n",
        "    index_md5    = int(hashlib.md5(item.encode(\"utf-8\")).hexdigest(),16) % (10 ** num_of_digits)\n",
        "    index_sha1   = int(hashlib.sha1(item.encode(\"utf-8\")).hexdigest(),16) % (10 ** num_of_digits)\n",
        "    index_sha512 = int(hashlib.sha512(item.encode(\"utf-8\")).hexdigest(),16) % (10 ** num_of_digits)\n",
        "    index_sha384 = int(hashlib.sha384(item.encode(\"utf-8\")).hexdigest(),16) % (10 ** num_of_digits)\n",
        "\n",
        "    return index_md5, index_sha1, index_sha512, index_sha384"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuWVB39kDgIO"
      },
      "source": [
        "##Build Bloom Filter\n",
        "Now we are ready to use the words from the dictionary to build the Bloom Filter. Note that in the original Bloom Filter algorithm introcuded in class, we used a bit array and set the indices to 1 to mark the existance of a valid element from the dictionary. Now we want to implemment it in a different way. Rather than return a bit arrary, we want to return all the valid indices instead. For example, assume that by using the orignial Bloom FIlter algorithm, we get a bit array (0, 0, 1, 0, 1, 1, 1, 0, 1, 1). Now we would like to return the indices of 1s, i.e., (2, 4, 5, 6, 8, 9).\n",
        "\n",
        "**You need to fill in the code in the function below.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpxScgyuGQeJ"
      },
      "source": [
        "def build_bloom_filter(in_data, num_of_digits):\n",
        "    \n",
        "    # Split the input file into multiple lines\n",
        "    in_split = in_data.flatMap(lambda x: x.split())\n",
        "    \n",
        "    # could only get it to work after converting it into a RDD from a PipelinedRDD \n",
        "    # I don't know what that didn't work otherwise\n",
        "    rdd = spark.sparkContext.parallelize(in_split.collect())\n",
        "\n",
        "\n",
        "    # Apply four different hash functions to create the Bloom filter\n",
        "    # Each one of these RDDs has the indice that will be set to 1 to mark the existance of the input elements\n",
        "\n",
        "    in_md5 =  rdd.map(lambda x: hash_values(x, num_of_digits)[0])\n",
        "    in_sha1 =  rdd.map(lambda x: hash_values(x, num_of_digits)[1])\n",
        "    in_sha512 =  rdd.map(lambda x: hash_values(x, num_of_digits)[2])\n",
        "    in_sha384 =  rdd.map(lambda x: hash_values(x, num_of_digits)[3])\n",
        "\n",
        "    \n",
        "    # Union all sets of indices, so we know the full collection of indices that have to be set to 1\n",
        "    in_allhashes = sc.union([in_md5,in_sha1,in_sha512,in_sha384])\n",
        "\n",
        "    # Remove duplicates\n",
        "    in_distinctIndex = in_allhashes.distinct()\n",
        "\n",
        "    # how to test to make sure it works\n",
        "\n",
        "\n",
        "    # non_unique_nums = spark.sparkContext.parallelize(in_allhashes.collect())\n",
        "    # print(non_unique_nums.takeOrdered(10, key=lambda x: x))\n",
        "    # nums = spark.sparkContext.parallelize(in_distinctIndex.collect())\n",
        "    # print(nums.takeOrdered(10, key=lambda x: x))\n",
        "\n",
        "\n",
        "    # Return all the indices of value 1\n",
        "    return in_distinctIndex\n",
        "\n"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EhWBSrONU4S"
      },
      "source": [
        "# call build_bloom_filter() function to get all the valid indices\n",
        "valid_indices = build_bloom_filter(word_rdd, num_of_digits)\n",
        "\n"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pVRpdRcHyzR"
      },
      "source": [
        "## Test set membership using Bloom Filter\n",
        "\n",
        "For each word in the test set, we need to determine is it is a valid word in the dictionary. We need to apply the four hash functions introcuded earlier, and determine if all four hash incides are valid indices.\n",
        "\n",
        "**You need to fill in the code in the function below.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfNGoHWvl8V3",
        "outputId": "5d63751e-3fd5-4ad0-cda2-e7f3c4a9c041"
      },
      "source": [
        "def bloom_filter(valid_indices, num_of_digits, test):\n",
        "\n",
        "    for item in test:\n",
        "        #print(type(valid_indices))\n",
        "        # Checks if an item exists in a set of indices for a bloom filter\n",
        "        index_md5, index_sha1, index_sha512, index_sha384 = hash_values(item, num_of_digits)\n",
        "\n",
        "        # Use filter() function to determine if each of the indices is a member of valid_indices\n",
        "        indexes = [index_md5, index_sha1, index_sha512, index_sha384]\n",
        "\n",
        "      # I think this is a cleaner way of doing it. \n",
        "        # this rdd is the index of the proper hash values. \n",
        "        # If there are 4 integers (eg the word has 4 valid indexes) then\n",
        "        # either it is in the set or there are 4 false positives on each of the hash functions. \n",
        "        # since this method makes certain the that members of the set pass\n",
        "        # as long as there are 4 elements in the set then is possibly passes. \n",
        "        # not equal to 4 is a certain fail. \n",
        "\n",
        "        indexes_found_in_valid = valid_indices.filter(lambda x: x in indexes)\n",
        "\n",
        "        all_found = (indexes_found_in_valid.count() == 4)\n",
        "\n",
        "        # Print message with results depending on the value of all_found\n",
        "        if all_found:\n",
        "            print(\"'\", item, \"' is possibly in the dictionary.\")\n",
        "        else:\n",
        "            print(\"'\", item, \"' is not in the dictionary for sure.\")\n",
        "\n",
        "\n",
        "\n",
        "bloom_filter(valid_indices, num_of_digits, test_set)\n"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "' I ' is possibly in the dictionary.\n",
            "' am ' is possibly in the dictionary.\n",
            "' nnot ' is not in the dictionary for sure.\n",
            "' good ' is possibly in the dictionary.\n",
            "' at ' is possibly in the dictionary.\n",
            "' speling ' is not in the dictionary for sure.\n",
            "' Pleasee ' is not in the dictionary for sure.\n",
            "' dont ' is possibly in the dictionary.\n",
            "' laught ' is not in the dictionary for sure.\n",
            "' at ' is possibly in the dictionary.\n",
            "' me ' is possibly in the dictionary.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctU1dYjfOif7"
      },
      "source": [
        "## Get Test Data\n",
        "Now we authenticate a Google Drive client to download files. Please follow the instruction to enter the authoriztion code.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dfnX7IAOkvH"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF5nuSdyTJpc"
      },
      "source": [
        "id='1QhWtO03km0oP8EBgEco2zvuLok7K5riI'\n",
        "downloaded = drive.CreateFile({'id': id}) \n",
        "downloaded.GetContentFile('test.txt') "
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKzWz2w1JSMi",
        "outputId": "0a5b3ce3-e402-44dc-cac7-b343f877617a"
      },
      "source": [
        "test_set = [ word for word in map(lambda x: x.strip(), open(\"test.txt\").readlines()) ]\n",
        "\n",
        "test_set\n"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'am',\n",
              " 'nnot',\n",
              " 'good',\n",
              " 'at',\n",
              " 'speling',\n",
              " 'Pleasee',\n",
              " 'dont',\n",
              " 'laught',\n",
              " 'at',\n",
              " 'me']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRT2geHpJZPx"
      },
      "source": [
        "##Apply Bloom Filter to the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vPokNGftJKi",
        "outputId": "cad8a895-4a90-4f5c-e0e3-12f9bf6e2b57"
      },
      "source": [
        "bloom_filter(valid_indices, num_of_digits, test_set)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "' I ' is possibly in the dictionary.\n",
            "' am ' is possibly in the dictionary.\n",
            "' nnot ' is not in the dictionary for sure.\n",
            "' good ' is possibly in the dictionary.\n",
            "' at ' is possibly in the dictionary.\n",
            "' speling ' is not in the dictionary for sure.\n",
            "' Pleasee ' is not in the dictionary for sure.\n",
            "' dont ' is possibly in the dictionary.\n",
            "' laught ' is not in the dictionary for sure.\n",
            "' at ' is possibly in the dictionary.\n",
            "' me ' is possibly in the dictionary.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}