{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ParkerBurchett Big Data Lab 2 LSH in spark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parkerburchett/BigDataClass/blob/main/ParkerBurchett_Big_Data_Lab_2_LSH_in_spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTwDFnjHMk25"
      },
      "source": [
        "#Colab 2: Implement Locality Sensitive Hashing using Spark\n",
        "\n",
        "In this lab you will learn to use [Apache Spark](https://spark.apache.org) on a Colab enviroment to implement Locality Sensitive Hashing for document comparisons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXQzA01OS_yQ"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbYZoVVWOZA5"
      },
      "source": [
        "Let's setup Spark on your Colab environment.  Run the cell below!\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhzk3GE6S9RC",
        "scrolled": true,
        "outputId": "73b7ddab-2e27-49b6-a90a-53c7b6159a32"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2MB 20kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 38.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612242 sha256=e0e20f775de0bfb840caa4fd64f1e0abcc32051a8834444b2bb99bca59cf2283\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n",
            "The following additional packages will be installed:\n",
            "  openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 2 newly installed, 0 to remove and 15 not upgraded.\n",
            "Need to get 36.5 MB of archives.\n",
            "After this operation, 143 MB of additional disk space will be used.\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "(Reading database ... 146456 files and directories currently installed.)\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u275-b01-0ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u275-b01-0ubuntu1~18.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u275-b01-0ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u275-b01-0ubuntu1~18.04) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u275-b01-0ubuntu1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u275-b01-0ubuntu1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctU1dYjfOif7"
      },
      "source": [
        "Now we authenticate a Google Drive client to download files. Please follow the instruction to enter the authoriztion code.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dfnX7IAOkvH"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq4vCnP5B5NX"
      },
      "source": [
        "##Data Sets\n",
        "\n",
        "We will work with the NIPS dataset of the [Bag of Words Data Set](https://archive.ics.uci.edu/ml/datasets/bag+of+words). It consists of two files:\n",
        "\n",
        "\n",
        "*   `docword.nips.txt` contain shingles of the document. It contains the count of each words in each document. Importantly, stop words were removed, and only words appearing more than 10 times are kept.\n",
        "*    `vocab.nips.txt` contains all the used words.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF5nuSdyTJpc"
      },
      "source": [
        "id='1831W_1SpE3A04SqsRtNiGZXhGfzZGFcl'\n",
        "downloaded = drive.CreateFile({'id': id}) \n",
        "downloaded.GetContentFile('vocab.nips.txt') \n",
        "\n",
        "id='1qjhteWSwdjIV7nbnDHa_0QgNEiM2YYI0'\n",
        "downloaded = drive.CreateFile({'id': id}) \n",
        "downloaded.GetContentFile('docword.nips.txt') \n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wA49WWqmO5rR"
      },
      "source": [
        "Now let's check if the stop words are trule removed from the vocabulary and let's see the first 10 words from the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzlEqVRLiRVF",
        "outputId": "762ea5f0-b2cf-4f34-914e-3332053a18cf"
      },
      "source": [
        "vocabulary = [ word for word in map(lambda x: x.strip(), open(\"vocab.nips.txt\").readlines()) ]\n",
        "\n",
        "# check some stop words\n",
        "print('the' in vocabulary or 'a' in vocabulary or 'to' in vocabulary)\n",
        "\n",
        "# example of shingles\n",
        "print(vocabulary[:20])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "['a2i', 'aaa', 'aaai', 'aapo', 'aat', 'aazhang', 'abandonment', 'abbott', 'abbreviated', 'abcde', 'abe', 'abeles', 'abi', 'abilistic', 'abilities', 'ability', 'abl', 'able', 'ables', 'ablex']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xb7Hv_vQLPGw"
      },
      "source": [
        "##Locality Sensitive Hashing\n",
        "Now let's implement locality sensitive hashing. Remeber, it involves three step:\n",
        "\n",
        "\n",
        "1.   Shingling: Convert the documents into sets (of words). This was already done. Since the data matrix after shingling is sparse, the shingling result is stored in `docword.nips.txt` as triples (`docID`, `wordID`, `count`).\n",
        "2.   Min-hashing: Use appropriate hash functions to convert the sets (of words) into signatures.\n",
        "3.   LSH: Generate candidate pairs and calculate the similarity between the candidates.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv_aXQDamh3p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f1925d5-bf82-4d3e-d10d-ab32ede3d137"
      },
      "source": [
        "#Import Spark libraries and initialize the Spark context.\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "\n",
        "# create the context\n",
        "#sc.stop()\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "print('created spark session')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "created spark session\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJqjlDWWouMc"
      },
      "source": [
        "###**Step 1: Shingling**\n",
        "The document was already shingled, so we only need to pre-process it. \n",
        "\n",
        "\n",
        "*   The `map()` function splits the input file by lines\n",
        "*   The `filter()` function only keeps the lines of 3 tokens\n",
        "* The next `map()` function retrieves the first two tokens because we don't care about the word count. Also, it changes the beginning word index from 1 (orginal setting in the data file) to 0.\n",
        "* The `groupByKey()` function groups all the words associted with the same document into a `ResultIterable` object.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TahXZgjElHZn"
      },
      "source": [
        "docrdd = sc.textFile (\"docword.nips.txt\") \\\n",
        "        .map(lambda line: line.split()) \\\n",
        "        .filter(lambda line: len(line) == 3) \\\n",
        "        .map(lambda y: (y[0], int(y[1]) - 1)) \\\n",
        "        .groupByKey()\n",
        "\n",
        "# to help you understand the structure of docrdd:\n",
        "#docrdd.map(lambda x : (x[0], list(x[1]))).take(2)   \n",
        "a_doc = docrdd.map(lambda x : (x[0], list(x[1]))).take(1)\n",
        "\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANY5q_Kf6A4A"
      },
      "source": [
        "###**Step 2: Min-hashing**\n",
        "Now you need to find the signature of each document using min-hash functions. You need to implement `get_signature()` function which takes one element (i.e., (`doc`, `list_of_words`)) in `docrdd` as input, and returns (`doc`, `signature`) as output. Your hash functions should be defined as `((a * x) + b) % len(vocabulary)` where `a` and `b` are random integers and `x` is a word's index. Let's create 100 hash functions using 100 random `a` and random `b`. This means the signature matrix should have 100 rows.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz-jG9_Rp5eK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e56d8822-9b22-40a0-c865-b59f5a31052a"
      },
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "SIGNATURE_SIZE = 100 \n",
        "# A and B are used to generate random permutations. \n",
        "A = random.sample(range(1,1500), SIGNATURE_SIZE) # these are random arrays of ints between 1 and 1500. It is 100 long\n",
        "B = random.sample(range(1,1500), SIGNATURE_SIZE)\n",
        "    \n",
        "\n",
        "def get_signature(p):\n",
        "  \n",
        "  doc,words = p\n",
        "  signature_list = [] \n",
        "  for i in range(SIGNATURE_SIZE):\n",
        "    a= A[i]\n",
        "    b= B[i]\n",
        "    min_hash_value = math.inf # start the hash_values at positive infinity\n",
        "    for word in words:\n",
        "      local_hash_value = (a*word +b) % 15000 # alters are 100 and 1500 I think there should be this many bands since this is the number of words that you are considering. \n",
        "      # you only want the shingles that are the same to hash into the same bands. Since there are 15000 unique shingles. this would be 15000?\n",
        "\n",
        "      if local_hash_value < min_hash_value: # only take the lowest hash_value.\n",
        "        min_hash_value = local_hash_value\n",
        "      \n",
        "    signature_list.append(min_hash_value)\n",
        "  \n",
        "  return(doc, tuple(signature_list)) # return the signatures for the current document doc\n",
        "\n",
        "\n",
        "local_sig = get_signature(a_doc[0])\n",
        "local_sig\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('1',\n",
              " (4,\n",
              "  45,\n",
              "  17,\n",
              "  39,\n",
              "  10,\n",
              "  10,\n",
              "  10,\n",
              "  9,\n",
              "  24,\n",
              "  4,\n",
              "  46,\n",
              "  1,\n",
              "  5,\n",
              "  18,\n",
              "  40,\n",
              "  7,\n",
              "  106,\n",
              "  34,\n",
              "  6,\n",
              "  43,\n",
              "  38,\n",
              "  28,\n",
              "  27,\n",
              "  17,\n",
              "  46,\n",
              "  50,\n",
              "  12,\n",
              "  11,\n",
              "  32,\n",
              "  0,\n",
              "  17,\n",
              "  4,\n",
              "  59,\n",
              "  7,\n",
              "  29,\n",
              "  10,\n",
              "  43,\n",
              "  547,\n",
              "  7,\n",
              "  23,\n",
              "  10,\n",
              "  57,\n",
              "  52,\n",
              "  37,\n",
              "  50,\n",
              "  11,\n",
              "  49,\n",
              "  4,\n",
              "  31,\n",
              "  13,\n",
              "  13,\n",
              "  22,\n",
              "  32,\n",
              "  18,\n",
              "  77,\n",
              "  11,\n",
              "  1,\n",
              "  44,\n",
              "  32,\n",
              "  28,\n",
              "  0,\n",
              "  4,\n",
              "  19,\n",
              "  17,\n",
              "  98,\n",
              "  25,\n",
              "  163,\n",
              "  19,\n",
              "  140,\n",
              "  139,\n",
              "  5,\n",
              "  33,\n",
              "  3,\n",
              "  7,\n",
              "  35,\n",
              "  7,\n",
              "  42,\n",
              "  13,\n",
              "  3,\n",
              "  22,\n",
              "  4,\n",
              "  96,\n",
              "  9,\n",
              "  6,\n",
              "  17,\n",
              "  35,\n",
              "  3,\n",
              "  1,\n",
              "  0,\n",
              "  35,\n",
              "  7,\n",
              "  5,\n",
              "  21,\n",
              "  40,\n",
              "  16,\n",
              "  5,\n",
              "  11,\n",
              "  8,\n",
              "  42,\n",
              "  34))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5vgydB0FEgv"
      },
      "source": [
        "###**Step 3: Generate Candidate Pairs**\n",
        "\n",
        "Now you need to use bands technique to find candidate pairs. You need to implement `hash_bands()` function which takes the signature from the previous step as input, split the signature into chunks, apply python system hash function `hash()` to the band, and return ((`band_index`, `hash_value`), `doc_id` ) key-value pairs. For example ((3, 472648357823), 1111) indicates that in Band #3, Document 1111 has the hash value of 472648357823. Why do we want to use (`band_index`, `hash_value`) as the key and use `doc_id` as the value? Because later we want to use Spark reduce() function to conbine all the documents that have the same hash value within the same band into the same list. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgQaRx6rSaf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "657325d8-2bcb-4e57-d0ef-9a7a89a4eb5c"
      },
      "source": [
        "NUM_BANDS = 10  # your can change it to different value\n",
        "\n",
        "def hash_bands(p):\n",
        "    result =[]\n",
        "    doc,sig = p   # sig is len 100. \n",
        "\n",
        "    band_size= int(len(sig)/NUM_BANDS) # this will be an integer. \n",
        "\n",
        "    sub_band = [sig[band_index*band_size:(band_index*band_size)+band_size] for band_index in range(NUM_BANDS)]\n",
        "    bands =[]\n",
        "    for i in range(len(sub_band)):\n",
        "      band_index = i\n",
        "      hash_value = hash(tuple(sub_band[i]))\n",
        "      bands.append(((band_index,hash_value),[doc]))\n",
        "\n",
        "    return bands\n",
        "\n",
        "b= hash_bands(local_sig)\n",
        "b\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((0, -7088801302551341579), ['1']),\n",
              " ((1, -2768857517409704263), ['1']),\n",
              " ((2, -619708399812700361), ['1']),\n",
              " ((3, 1626970108689554962), ['1']),\n",
              " ((4, -5972919618820813069), ['1']),\n",
              " ((5, 8223438051271834461), ['1']),\n",
              " ((6, -5052195376791979475), ['1']),\n",
              " ((7, 2889897081346053594), ['1']),\n",
              " ((8, -6657880895385938339), ['1']),\n",
              " ((9, 2129975254363074805), ['1'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbtIidEO-XAI"
      },
      "source": [
        "##Put It All Together\n",
        "\n",
        "Now you are ready to generate candidate pairs.\n",
        "\n",
        "* In `map()` function, the `get_signature` returns the document and a set of signatures for that document\n",
        "*In `flatmap()` function, the `hash_bands` first splits the signatures into chunks. Then a system hash function is applied to the signature to get a hash value. `hash_bands` returns (key, value) pairs. The key is (`band_index`, `hash_value`); the value is the `doc_id`.\n",
        "\n",
        "*   In `reduceByKey()` function, if two documents share the same key, they are combined together. Remember key is (`band index`, `hash_value`); if they have the same key, this indicates these two documents have the same signature within that band.\n",
        "*   The `filter()` function only keeps the (key, value\\*) pairs when value\\* has more than one document.\n",
        "\n",
        "**Note: the result you obtained might be different from the one listed below.** Why? Becaues we can't control those random numbers. If you believe your code is correct, but you can't get any candidate pairs, try to tune `NUM_BANDS` variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ft3VivrSagB",
        "outputId": "4eea35a0-004a-42df-a0d0-4c652cf3a9e0"
      },
      "source": [
        "candidates = docrdd \\\n",
        "    .map(get_signature) \\\n",
        "    .flatMap(hash_bands) \\\n",
        "    .reduceByKey(lambda a, b: a + b) \\\n",
        "    .filter(lambda v: len(v[1]) > 1) \\\n",
        "    .collect()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "candidates"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((5, -8264822105078015641), ['581', '626']),\n",
              " ((9, 8721454960800295353), ['883', '1176']),\n",
              " ((7, -8006132156591484569), ['149', '872']),\n",
              " ((0, 8026840134662814200), ['731', '921'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdjzFNw_RBT6"
      },
      "source": [
        "I got two candidate pairs. Now let's calcuate the Jaccard similary between candidate pairs. You may get more than two pairs. Pick any pair you would like to test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD7rfKauRiJg"
      },
      "source": [
        "Get the word list in the first document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Bwt84b_Fzrs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "631bd984-bec8-41ff-f5f6-debf097d778a"
      },
      "source": [
        "a1 = list(docrdd.filter(lambda x: x[0] == '581').first()[1]) #change doc_id\r\n",
        "a1[:10]"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[38, 75, 76, 94, 97, 139, 157, 170, 171, 173]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtPHzN48Rmgw"
      },
      "source": [
        "Get the word list in the second document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWhT2IQFO-mV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db238484-ba8f-49a7-dcdc-1de71f60911a"
      },
      "source": [
        "a2 = list(docrdd.filter(lambda x: x[0] == '626').first()[1]) #change doc_id\r\n",
        "a2[:10]"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[17, 38, 88, 94, 98, 104, 149, 153, 157, 188]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lXIOvW4R1DM"
      },
      "source": [
        "Calculate the Jaccard similarity by finding out the set intersection and the set union of the above two word lists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXq2h4B0PgQX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98a32ce3-d6b9-4b31-dc96-bf11fc4f663e"
      },
      "source": [
        "intersect = set(a1) & set(a2)\n",
        "union = set(a1 + a2)\n",
        "sim = len(intersect)/len(union)\n",
        "sim"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1528117359413203"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzMwh9WTU_OK"
      },
      "source": [
        "This pair of documents has the similarity of 0.15539305301645337. Yours should be close to this value."
      ]
    }
  ]
}